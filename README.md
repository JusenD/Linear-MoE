
<div align="center">

# Linear-MoE

[![hf_model](https://img.shields.io/badge/ðŸ¤—-Models-blue.svg)](https://huggingface.co/xxx) | [![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white)](https://discord.gg/xxx)
</div>

This repo provides a **production-level modeling and training framework** based on the latest [Megatron-Core](https://github.com/NVIDIA/Megatron-LM), for the SOTA open-source MoE models integrated with linear sequence modeling methods (Linear Attention, State Space Modeling, Linear RNN). **Any pull requests are welcome!**

<!-- <div align="center">
  <img width="400" alt="image" src="https://github.com/xxx">
</div> -->

# Models

|     |   |  Qwen2 MoE (@Alibaba)  |    Deepseek v2 MoE (@Deepseek)       |    Mixtral MoE (@Mistral AI)   |         |        |
| :-----: | :----------------------------: | :----------------------------: | :---------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------: |
| Linear Attention |       [RetNet](https://arxiv.org/abs/2307.08621) (@MSRA@THU)       | Y |          Y          |     Y      |                 |  |
| Linear Attention |         [GLA](https://arxiv.org/abs/2312.06635) (@MIT@IBM)         | Y |     Y      |    Y       |                              |     |
| Linear Attention |         [Lighting Attention](https://arxiv.org/abs/2405.17381)(@Shanghai AI Lab)         | |   | |                      |         |
| Linear Attention | [Based](https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based) (@Stanford@Hazyresearch) |  |            |           |    |   |
| Linear Attention |            [Rebased](https://arxiv.org/abs/2402.10644) (@Tinkoff)            | |    |           |         |    |
| Linear Attention |           [Delta Net](https://arxiv.org/abs/2102.11174) (@MIT)            | |        |           |             |      |
| SSM |             [Mamba2](https://arxiv.org/abs/2405.21060) (@Princeton@CMU)              | Y | Y  |   Y   |          |       |
| Linear RNN |             [RWKV6](https://arxiv.org/abs/2404.05892) (@RWKV)              |  Y  |   Y   |    Y    |          |        |
| Linear RNN |             [HGRN2](https://arxiv.org/abs/2404.07904) (@TapTap@Shanghai AI Lab)             | Y |   Y   |   Y   |          |      |
<!-- | Linear Attention |    [Hedgehog](https://openreview.net/forum?id=4g02l2N2Nx) (@HazyResearch)    | |      |      |         |    | 
<!-- | Linear Attention | [PolySketchFormer](https://arxiv.org/abs/2310.01655) (@CMU@Google) | |      |          |      |       | 

<!-- | Linear Attention |       [RWKV-v4](https://arxiv.org/abs/2305.13048) (@BlinkDL)       | |               |         |           |        |
<!-- | Linear Attention |            [GateLoop](https://arxiv.org/abs/2311.01927)            | |         |   |            | -->
<!-- | Linear Attention |           [ABC](https://arxiv.org/abs/2110.02488) (@UW)            | |            |          |            |       |
<!-- | Linear Attention |         [VQ-transformer](https://arxiv.org/abs/2309.16354)         | |         |          |                |         |
| Linear RNN |              [HGRN](https://openreview.net/forum?id=P1TCHxJwLB)              | |        |     |        |      |
<!-- | SSM |             [Samba](https://arxiv.org/abs/2406.07522)              | | |            |           |       | -->





# Installation

- [PyTorch](https://pytorch.org/) >= 2.0
- [Triton](https://github.com/openai/triton) >=2.2
- [einops](https://einops.rocks/)


# Usage

## Pretraining

## Finetuning

## Generation

## Evaluation


<!-- # Different forms of linear attention

Please refer to Sectiton 2.3 of [GLA paper](https://arxiv.org/pdf/2312.06635.pdf) for hardware considerations of different forms of linear attention.

* `Parallel`: Self-attention-styled computation in $O(L^2)$ time with sequence parallelism.
* `FusedRecurrent`: Recurrent computation in $O(L)$ time. Hidden states are computed on-the-fly in shared memory without any materialization to global memory (see Algorithm1 of [this paper](https://arxiv.org/pdf/2006.16236.pdf) for more details!). This saves a lot of I/O cost and should be a strong baseline for speed comparison.
* `FusedChunk`: Chunkwise computation in $O(LC)$ time where $C$ is the chunk size. Hidden states are computed on-the-fly without any materialization to global memory likewise **FusedRecurrent**. This version is usually better than FusedReuccurent because tensor cores can be used for sequence level "reduction", whilst FusedRecurrent cannot use tensor cores at all.  Note that there is no sequence level parallelism in this implementation, so this impl is not suitable for the very small batch size setting. Should be more memory efficient than ParallelChunk. 
* `ParallelChunk`: Chunkwise computation with sequence parallelism. Need to materialize hidden states to global memory for each chunk. $C$ is needed to set properly to achieve good performance because when $C$ is small there are too many hidden states to load/store to global memory; and when $C$ is too large the FLOPs are high. Recommened $C$ is [64, 128, 256] -->


# Citation
<!-- If you find this repo useful, please consider citing our works:
```bib

``` -->